{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor classification in detail\n",
    "We already saw how to use the nearest neighbors classifier that is provided by scikit-learn. In this lab we will implement the algorithm ourselves to get a better understanding of it's inner workings.\n",
    "\n",
    "In practice, you are likely to make use of existing implementations for most modeling work. However, it can be helpful to look behind the curtains and see that the algorithms are not necessarily complicated black boxes, but that you could implement them yourself as well.\n",
    "\n",
    "We will also look at the influence of the hyper parameters in the nearest neighbors model, the number of neighbors to consider, in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy dataset\n",
    "We are starting with a two-dimensional toy dataset, that is a dataset with two features. Looking at datasets with two feature can be helpful for illustration purposes as we can visualize the data using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(noise=.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Start by splitting the data into a training and a test set.\n",
    "We will start by creating a \"one nearest neighbor\" model that only takes the one nearest neighbor into account.\n",
    "\n",
    "Compute the euclidean distances from all data points in the test data to all the data points in the training data in a numpy array. You can either compute the distances directly with numpy, use the ``sklearn.metrics.pairwise`` module, or use ``scipy.spatial``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute distances here\n",
    "# distances = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, compute the index of the closest point in the training data using ``np.argmin``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closest_for_each_test_point = np.argmin(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, compute the predictions ``y_pred`` made by the model, by assigning to each test point the label of the closest training point in ``y_train``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute the accuracy of our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compare the predictions made by your code with the predictions made by ``KNeighbors`` with ``n_neighbors=1``. They should be identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, extend your code to take multiple neighbors into account. You can use ``np.argsort`` to find the ``k`` nearest neighbors (say 5) from the distances. Fetch the label associated with all 5 neighbors, and compute the most common label among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your results again with those obtained using ``KNeighborsClassifier``, this time with ``n_neighbors=5``. Again they should be identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influence of n_neighbors in KNeighborsClassifier\n",
    "Next, let's study how changing the number of neighbors to consider changes the predictions. One way to inspect a model is to look at the **decision boundaries**, which is the boundaries in the input space between data that is classifed as one class and data that is classified as another class.\n",
    "As our data only has two features, we can easily visualize these boundaries in the input space. The following function can help you with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_classification(classifier, X):                                       \n",
    "                                                                                                            \n",
    "    eps = X.std() / 2.                                                                                                       \n",
    "    ax = plt.gca()                                                                                                           \n",
    "    \n",
    "    # create a dense grid of values to evaluate the classifier\n",
    "    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps                                                                      \n",
    "    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps                                                                      \n",
    "    xx = np.linspace(x_min, x_max, 1000)                                                                                         \n",
    "    yy = np.linspace(y_min, y_max, 1000)                                                                                         \n",
    "    X1, X2 = np.meshgrid(xx, yy)                                                                                                 \n",
    "    X_grid = np.c_[X1.ravel(), X2.ravel()]\n",
    "    # evaluate the classifier on the grid\n",
    "    decision_values = classifier.predict(X_grid)\n",
    "    # show the classification results:\n",
    "    ax.imshow(decision_values.reshape(X1.shape), extent=(x_min, x_max,                                                           \n",
    "                                                       y_min, y_max),                                                          \n",
    "            aspect='auto', origin='lower', alpha=alpha)                                                               \n",
    "    ax.set_xlim(x_min, x_max)                                                                                                    \n",
    "    ax.set_ylim(y_min, y_max)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Use the ``plot_2_classification`` function to plot the decision boundary for the toy data we used above for ``KNeighborsClassifier`` with different values of ``n_neighbors``. What do you notice?\n",
    "For each value of ``n_neighbors`` you try, also record training set accuracy and test set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
