{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation and model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston, load_digits\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threefold split and parameter search\n",
    "The simplest way to adjust parameters is to split the data into three parts: a training, a validation and a test set.\n",
    "For each parameter setting, we fit a model on the training set, and evaluate it on the evaluation set.\n",
    "We select the \"best\" parameter setting (or model) based on the validation set. We then rebuild a model using training and\n",
    "validation data with this parameter setting, and evaluate it on the test set. The test set performance serves as an estimate of the generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Load the boston housing data. Split the data into three parts, for example by calling ``train_test_split`` twice.\n",
    "As yesterday, scale the data and create polynomial features.\n",
    "Search the best setting for the regularization parameter alpha using the strategy described above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0.001     0.01      0.1       1.       10.      100.     1000.   ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "alphas = np.logspace(-3, 3, 7)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing parameter search...\n",
      "       alpha        score\n",
      "    0.001000     0.698127\n",
      "    0.010000     0.698149\n",
      "    0.100000     0.698359\n",
      "    1.000000     0.700425\n",
      "   10.000000     0.717783\n",
      "  100.000000     0.759543\n",
      " 1000.000000     0.568988\n",
      "maximum score =     0.759543\n",
      "chosen alpha  =   100.000000\n",
      "Performing fit on entire training set...\n",
      "Peforming test...\n",
      "score =     0.583461\n"
     ]
    }
   ],
   "source": [
    "X_trainval, X_test, y_trainval, y_test = train_test_split(boston.data, boston.target, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_trainval = scaler.transform(X_trainval)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Performing parameter search...\")\n",
    "scores = []\n",
    "print(\"{0:>12s} {1:>12s}\".format(\"alpha\",\"score\"))\n",
    "for a in alphas:\n",
    "    ridge = Ridge(a)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    score = ridge.score(X_val, y_val)\n",
    "    scores.append(score)\n",
    "    print(\"{0:12.6f} {1:12.6f}\".format(a,score))\n",
    "    \n",
    "print(\"maximum score = {:12.6f}\".format(scores[np.argmax(scores)]))\n",
    "myalpha = alphas[np.argmax(scores)]\n",
    "print(\"chosen alpha  = {:12.6f}\".format(myalpha))\n",
    "\n",
    "print(\"Performing fit on entire training set...\")\n",
    "ridge = Ridge(myalpha)\n",
    "ridge.fit(X_trainval, y_trainval)\n",
    "print(\"Peforming test...\")\n",
    "score = ridge.score(X_test, y_test)\n",
    "print(\"score = {:12.6f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "To get a better understanding of cross-validation, we'll implement it from scratch.\n",
    "Our goal is to estimate the performance of a single model, let's say ``Ridge(alpha=1)`` on the original Boston housing dataset.\n",
    "\n",
    "### Task 2\n",
    "Complete the code below to fit a model for each of the folds of 5-fold cross-validation and compute the hold-out $R^2$ using the ``score method``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold out the following range: 0-101\n",
      "[-- -- -- -- -- -- -- -- -- -- -- -- --]\n",
      "[0.13262 0.0 8.56 0.0 0.52 5.851 96.7 2.1069 5.0 384.0 20.9 394.05 16.47]\n",
      "0.742522041858\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "score =     0.742522\n",
      "Hold out the following range: 101-202\n",
      "[0.13262 0.0 8.56 0.0 0.52 5.851 96.7 2.1069 5.0 384.0 20.9 394.05 16.47]\n",
      "[-- -- -- -- -- -- -- -- -- -- -- -- --]\n",
      "0.742522041858\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "score =     0.742522\n",
      "Hold out the following range: 202-303\n",
      "[-- -- -- -- -- -- -- -- -- -- -- -- --]\n",
      "[0.13262 0.0 8.56 0.0 0.52 5.851 96.7 2.1069 5.0 384.0 20.9 394.05 16.47]\n",
      "0.742522041858\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "score =     0.742522\n",
      "Hold out the following range: 303-404\n",
      "[-- -- -- -- -- -- -- -- -- -- -- -- --]\n",
      "[0.13262 0.0 8.56 0.0 0.52 5.851 96.7 2.1069 5.0 384.0 20.9 394.05 16.47]\n",
      "0.742522041858\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "score =     0.742522\n",
      "Hold out the following range: 404-505\n",
      "[-- -- -- -- -- -- -- -- -- -- -- -- --]\n",
      "[0.13262 0.0 8.56 0.0 0.52 5.851 96.7 2.1069 5.0 384.0 20.9 394.05 16.47]\n",
      "0.742522041858\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "score =     0.742522\n",
      "[0.7425220418579892, 0.7425220418579892, 0.7425220418579892, 0.7425220418579892, 0.7425220418579892]\n"
     ]
    }
   ],
   "source": [
    "# TODO: problem with masking arrays\n",
    "\n",
    "X = boston.data[:505]  # we make it divisible by n_folds to make the code simpler\n",
    "y = boston.target[:505]\n",
    "scores = []\n",
    "n_folds = 5\n",
    "n_samples = len(X)\n",
    "fold_size = n_samples / n_folds\n",
    "alpha = 1.0\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    \n",
    "    X_hold_out_mask = np.zeros(X.shape, dtype=np.bool)\n",
    "    y_hold_out_mask = np.zeros(y.shape, dtype=np.bool)\n",
    "    \n",
    "    # assign True to the samples that are supposed to be held out in this fold\n",
    "    b = int((fold)*fold_size)\n",
    "    e = int((fold+1)*fold_size)\n",
    "    print(\"Hold out the following range: {0}-{1}\".format(b,e))\n",
    "    X_hold_out_mask[b:e,:] = True\n",
    "    y_hold_out_mask[b:e] = True\n",
    "    X_training_mask = ~X_hold_out_mask  # training data is inverse of hold out data\n",
    "    y_training_mask = ~y_hold_out_mask\n",
    "\n",
    "    # assign training and hold-out portions\n",
    "    X_train = np.ma.masked_array(X, X_hold_out_mask)\n",
    "    y_train = np.ma.masked_array(y, y_hold_out_mask)\n",
    "    X_test = np.ma.masked_array(X, X_training_mask)\n",
    "    y_test = np.ma.masked_array(y, y_training_mask)\n",
    "    \n",
    "    # FIXME - masking works here\n",
    "    print(X_test[105,:])\n",
    "    print(X_train[105,:])\n",
    "\n",
    "    # build model - scaling doesn't seem to be using masks!!\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    ridge = Ridge(alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    print(ridge.score(X_train, y_train))\n",
    "\n",
    "    # FIXME - but masking is now gone!\n",
    "    print(X_test[105,:])\n",
    "    print(X_train[105,:])\n",
    "\n",
    "    # compute scores\n",
    "    score = ridge.score(X_test_scaled, y_test)\n",
    "    print(\"score = {:12.6f}\".format(score))\n",
    "    scores.append(score)\n",
    "    \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "Compare the result of your implementation with the result of the ``cross_val_score`` method in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.60046754  0.52752528 -1.05360152]\n"
     ]
    }
   ],
   "source": [
    "X_train = X\n",
    "y_train = y\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "ridge = Ridge(1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "scores_sklearn = cross_val_score(ridge,X_train,y_train) # fill in missing arguments\n",
    "\n",
    "# compare scores_sklearn with scores\n",
    "print(scores_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter selection with cross-validation\n",
    "### Task 4\n",
    "Implement the same search over the parameter ``alpha`` in ``Ridge`` that you did in Task 1, but instead of splitting the data three times use cross-validation.\n",
    "In more detail:\n",
    "- Split the Boston housing data (with polynomial features) into two parts, training and testing\n",
    "- Loop over different values of alpha\n",
    "- for each value of alpha, call ``cross_val_score`` on the training set, and compute the mean cross-validated accuracy.\n",
    "- Select the parameter with the best mean crossvalidation accuracy, and build a model on all of the training data\n",
    "- evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV\n",
    "Because searching for the parameters of a model is such a common task, scikit-learn provides ``GridSearchCV`` which implements the procedure from Task 4 (with some bells an whistles).\n",
    "To use ``GridSearchCV`` we simply have to define a parameter grid to search as a dictionary, with the key the name of the parameter, and the values the parameters we like to try. The ``GridSearchCV`` class has the same interface as the classification and regression models, and we can call ``fit`` to perform the grid-search with cross-validation. It even refits the model using the best parameters! We can then use ``predict`` or ``score`` to use the model with the best parameters, retrained on the whole training data.\n",
    "\n",
    "### Task 5\n",
    "Do the same search from Task 4 (and Task 1) again, this time using ``GridSearchCV`` (from the ``sklearn.model_selection`` module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-3, 3, 7)\n",
    "param_grid = {'alpha':  alphas}\n",
    "\n",
    "grid = GridSearchCV( ,return_train_score=True) # complete me!\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``GridSearchCV`` object stored a lot of useful information from the grid-search in the ``cv_results_`` attribute.\n",
    "The easiest way to access it is to convert it to a pandas datafram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even plot the cross-validation accuracies and their associated uncertainties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot('param_n_neighbors', 'mean_train_score')\n",
    "results.plot('param_n_neighbors', 'mean_test_score', ax=plt.gca())\n",
    "plt.fill_between(results.param_n_neighbors.astype(np.int),\n",
    "                 results['mean_train_score'] + results['std_train_score'],\n",
    "                 results['mean_train_score'] - results['std_train_score'], alpha=0.2)\n",
    "plt.fill_between(results.param_n_neighbors.astype(np.int),\n",
    "                 results['mean_test_score'] + results['std_test_score'],\n",
    "                 results['mean_test_score'] - results['std_test_score'], alpha=0.2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "Select the best value of ``n_neighbors`` for using ``KNeighborsClassifier`` on the ``digits`` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics and scoring\n",
    "\n",
    "In this section, we'll look at different evaluation metrics in scikit-learn and how to use them.\n",
    "There's two main ways to use metrics:\n",
    "- As functions in the ``sklearn.metrics`` module, such as ``accuracy_score`` and ``roc_auc``. These take the true labels and the predictions as arguments.\n",
    "- By specifying a metrics in ``cross_val_score``, ``GridSearchCV`` or another evaluation method using the ``scoring`` keyword, i.e. ``cross_val_score(..., scoring='roc_auc')``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for binary classification\n",
    "As we mentioned, accuracy is not a great metric in imbalanced classification problems.\n",
    "We'll look at some alternatives.\n",
    "\n",
    "### Task 7\n",
    "Create an imbalanced classification problem from the digits dataset by classifying the digit 4 against all other digits.\n",
    "Split the data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ....\n",
    "# create X_train, X_test, y_train, y_test for \"4 vs rest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a ``LogisticRegression`` model, a ``DummyClassifier(strategy='most_frequent')`` and a ``DecisionTreeClassifier(max_depth=2)``, and compare their test-set accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lr = LogisticRegression()\n",
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "# build models\n",
    "# compare them using accuracy (for example using .score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better picture, now use the ``classification_report`` function from ``sklearn.metrics``:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report provides precision and recall for the default threshold. To look at all possible thresholds, we can plot the precision-recall curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_probs_lr = lr.predict_proba(X_test)[:, 1]\n",
    "# complete:\n",
    "# positive_probs_tree = tree.\n",
    "# plot curves for tree and logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a summary by computing the average precision (``average_precision_score``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to use something like ``average_precision_score`` in cross-validation, we can simply specify the ``scoring`` argument of ``cross_val_score``. Use ``cross_val_score`` to compute the 5 fold cross-validated average precision of ``LogisticRegression`` and ``DecisionTreeClassifier(max_depth=2)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... solution here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
