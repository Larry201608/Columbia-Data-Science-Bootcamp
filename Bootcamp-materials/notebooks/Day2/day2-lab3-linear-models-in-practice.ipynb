{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear models in practice\n",
    "In this lab session we will go over some important aspects of using linear models (and to some degree also neighborhood based models) in practice.\n",
    "In particular, we will do some simple preprocessing and feature engineering. We will use the Boston housing dataset for this again.\n",
    "\n",
    "## Data Scaling\n",
    "Before we get started, we want to look at the data in more detail (you can look at ``boston.DESCR`` if you need a reminder about the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pypslot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X, y = boston.data, boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Fill in the code below to create a scatter plot of each variable against the target. These univariate relationships can provide some insight into how to best model the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(20, 10))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    # plot the i-th feature in X against the target\n",
    "    # label the x-axis\n",
    "    # your code goes here...\n",
    "    ax.set_ylabel(\"MEDV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that for some features there is a clear dependence between the feature and the target. We can also see that the distributions of the different variables are quite different, as well as the ranges. We can see the difference between the ranges even more clearly if we do a box-plot of the features.\n",
    "\n",
    "### Task\n",
    "Create a boxplot of X. Make sure the axes are labeled properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution here ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having features of different orders of magnitude can be very detrimental for many models, for example regularized linear models and neighborhood based models - for linear regression without regularization, scaling of the input usually makes no difference, but can be benefitial to get a more stable solution.\n",
    "\n",
    "There is a regression equivalent to ``KNeighborsClassifier``, called ``KNeighborsRegressor``. It simply predicts the mean of the ``n_samples`` closest training points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Split the boston housing data into training and test set, apply the ``KNeighborsRegressor`` and compute the test set $R^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is not very acurate and the problem should be clear from the box plot above: the TAX feature has a much larger magnitude than any other feature. That means this feature will completely dominate which data points are considered neighbors, and all other features will have little influence. A simple solution is to scale all the features to have the same scale. A common choice is to scale all features to zero mean and unit variance. We can do this with the ``StandardScaler`` from the ``sklearn.preprocessing`` module. It's important that we compute the mean and standard deviation of the data only on the training set, and then use these estimates for both the training and the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Use the ``StandardScaler`` to rescale the Boston housing dataset. You can estimate the mean and variance by calling the ``fit`` method with the training data ``X_train`` (``y_train`` is not required). You can then scale the data using the ``transform`` method.\n",
    "Store the scaled training data in to ``X_train_scaled`` and the scaled test data into ``X_test_scaled``. Compute mean and variance for both ``X_train_scaled`` and ``X_test_scaled``. Are they what you expect?\n",
    "\n",
    "Now use the ``KNeighborsRegressor`` on the scaled data. The results should improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regulariation\n",
    "Coming back to linear models, we want to see what the effect of different amounts of regularization in a linear model is. Remember that the l2-penalized linear regression is called ``Ridge``.\n",
    "\n",
    "### Task\n",
    "Fit the Ridge model with the different values of ``alpha`` that are given. Record the ``R^2`` on the training and test set, and create a line plot comparing the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-3, 3, 7)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(alphas)2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    # instantiate ridge with this particular alpha\n",
    "    # code here ...\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    # compute training and test score, store them in train_scores and tests_scores\n",
    "    # code here...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train_scores and test_scores here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case small values of alpha lead to the best training and test performances. This means little regularization is beneficial, and we could have also just used ``LinearRegression``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Features\n",
    "Looking at the univariate plots of features vs target that we did in the beginning of this lab, it's clear that not all the relationships between features and target are linear. The relationship for ``RM`` looks somewhat linear, but the relationship for ``LSTAT`` is clearly not.\n",
    "There is several ways to deal with non-linear relationships. A particular simple one is adding polynomials of the original features as additional features, so for example adding $\\text{LSTAT}^2$. We will also add interactions between features, which further increases the power of the model.\n",
    "Both of these are implemented in the ``PolynomialFeatures`` transformation in ``sklearn.preprocessing``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Transform the scaled data ``X_train_scaled`` and ``X_test_scaled`` with the ``PolynomialFeatures`` transformation.\n",
    "How many features does transformed data have? Why?\n",
    "\n",
    "Build a ridge model using the scaled data. Is it worth or better than the model we build before?\n",
    "\n",
    "Now repeat the exercise from above and investigate the influence of different values of ``alpha`` on the model. How does the plot look different than before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables\n",
    "Another common issue in working with linear models and related models is dealing with categorical variables. Linear models can't by themselves deal with categorical variables, but a simple way around that is using dummy variables, also known as one-hot encoding. Let's run through a small example with ``pandas``. Imagine you have a dataset with an integer column ``salary`` and a categorical column ``boro`` that takes the value of a boro of Manhattan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'salary': [103, 89, 142, 54, 63, 219],\n",
    "                   'boro': ['Manhatten', 'Queens', 'Manhatten', 'Brooklyn', 'Brooklyn', 'Bronx']})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a one-hot representation of the data, we can simply call the ``get_dummies`` function in pandas. It will represent each categorical feature as several new features, one for each category. All values of the features will be zero, except for the one feature representing the category assigned to this datapoint. So there will always be exactly one entry of \"1\" for each group of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default pandas encodes all variables that contains strings, objects, or categories (which is a pandas concept that we won't explain here - in this example we simply use strings). Sometimes you might encounter data in which someone has already \"helpfully\" encoded categories as integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'salary': [103, 89, 142, 54, 63, 219],\n",
    "                   'boro': [0, 1, 0, 2, 2, 3]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling ``get_dummies`` on this data will not do anything:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform a categorical feature that's already encoded as an integer, you can pass it explicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df, columns=['boro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "Apply dummy encoding and scaling to the \"adult\" dataset consisting of income data from the 1990s census.\n",
    "The goal is to predict whether a person will make less or more than \\$50k a year, so this is a binary classification problem.\n",
    "Use logistic regression on the problem.\n",
    "\n",
    "Bonus: identify important features and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://github.com/amueller/ml-training-advanced/raw/master/notebooks/data/adult.csv\", index_col=0)\n",
    "# solution here ..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
