{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks with scikit-learn\n",
    "There is a basic implementation of neural networks in scikit-learn than you can use for prototyping and getting familiar with the concepts. Because it doesn't have GPU support, it's not really appropriate for larger applications.\n",
    "\n",
    "Neural networks are implemented in the ``MLPClassifier`` and ``MLPRegressor`` classes in the ``sklearn.neural_networks`` module.\n",
    "There are two standard solvers implemented: ``'adam'`` (the default) and ``'l-bfgs'``. ``'l-bfgs'`` might work better on very small toy datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Create a synthetic \"two moons\" dataset, and visualize the decision boundary when varying the number of hidden units, the non-linearity, and the random state.\n",
    "What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_classification(classifier, X, ax=None):                                       \n",
    "                                                                                                            \n",
    "    eps = X.std() / 2.\n",
    "    if ax is None:\n",
    "        ax = plt.gca()                                                                                                           \n",
    "    \n",
    "    # create a dense grid of values to evaluate the classifier\n",
    "    x_min, x_max = X[:, 0].min() - eps, X[:, 0].max() + eps                                                                      \n",
    "    y_min, y_max = X[:, 1].min() - eps, X[:, 1].max() + eps                                                                      \n",
    "    xx = np.linspace(x_min, x_max, 1000)                                                                                         \n",
    "    yy = np.linspace(y_min, y_max, 1000)                                                                                         \n",
    "    X1, X2 = np.meshgrid(xx, yy)                                                                                                 \n",
    "    X_grid = np.c_[X1.ravel(), X2.ravel()]\n",
    "    # evaluate the classifier on the grid\n",
    "    decision_values = classifier.predict(X_grid)\n",
    "    # show the classification results:\n",
    "    ax.imshow(decision_values.reshape(X1.shape), extent=(x_min, x_max,                                                           \n",
    "                                                       y_min, y_max),                                                          \n",
    "            aspect='auto', origin='lower', alpha=.5)                                                               \n",
    "    ax.set_xlim(x_min, x_max)                                                                                                    \n",
    "    ax.set_ylim(y_min, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "mlp = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(10,), random_state=0).fit(X_train, y_train)\n",
    "# todo: create subplots with multiple random states, different non-linearities and different numbers of hidden units\n",
    "plot_2d_classification(mlp, X)\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=60, edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Apply ``MLPRegressor`` to the boston housing dataset. Does it do better or worse than Ridge (yesterday).\n",
    "\n",
    "Try the model with and without scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=0)\n",
    "\n",
    "# ... solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "x = np.sort(rng.uniform(size=100))\n",
    "y = np.sin(10 * x) + 5 * x + np.random.normal(0, .3, size=100)\n",
    "plt.plot(x, y, 'o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line = np.linspace(0, 1, 100)\n",
    "X = x.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp_relu = MLPRegressor(solver=\"lbfgs\").fit(X, y)\n",
    "mlp_tanh = MLPRegressor(solver=\"lbfgs\", activation='tanh').fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o')\n",
    "plt.plot(line, mlp_relu.predict(line.reshape(-1, 1)), label=\"relu\")\n",
    "plt.plot(line, mlp_tanh.predict(line.reshape(-1, 1)), label=\"tanh\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data / 16., digits.target, stratify=digits.target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter=1000, random_state=0).fit(X_train, y_train)\n",
    "print(mlp.score(X_train, y_train))\n",
    "print(mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras\n",
    "Now let's work with the much more flexible and efficient Keras library (which is using TensorFlow in the backend)\n",
    "A simple way to work with keras is to build up models using the ``Sequential`` class.\n",
    "\n",
    "The model below corresponds to mulinomial logistic regression. Run it on the ``digits`` dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "# We divide the data by 16 to make sure it's on a reasonable scale (between zero and one)\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data / 16., digits.target, random_state=0)\n",
    "\n",
    "import keras\n",
    "num_classes = 10\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(10, input_shape=(64,)),\n",
    "    Activation('softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In keras, the ``fit`` method takes options like the number of epochs (iterations) to run, and whether to use a validation set for early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=10, verbose=1, validation_split=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss: {:.3f}\".format(score[0]))\n",
    "print(\"Test Accuracy: {:.3f}\".format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a nice way to visualize the history of training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history(logger):\n",
    "    df = pd.DataFrame(logger.history)\n",
    "    df[['acc', 'val_acc']].plot()\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    df[['loss', 'val_loss']].plot(linestyle='--', ax=plt.twinx())\n",
    "    plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history_callback.history)\n",
    "df[['acc', 'val_acc']].plot()\n",
    "plt.ylabel(\"accuracy\")\n",
    "df[['loss', 'val_loss']].plot(linestyle='--', ax=plt.twinx())\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Add an additional hidden layer with a ``relu`` non-linearity and fit the new model (``Dense(20, activation='relu')``. Experiment with different layer sizes and different numbers of additional layers."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
