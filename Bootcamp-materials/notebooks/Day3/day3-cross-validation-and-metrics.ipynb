{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation and model selection\n",
    "\n",
    "Task 3 has problems.\n",
    "Tasks 6 and 7 are not complete.\n",
    "-W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston, load_digits\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threefold split and parameter search\n",
    "The simplest way to adjust parameters is to split the data into three parts: a training, a validation and a test set.\n",
    "For each parameter setting, we fit a model on the training set, and evaluate it on the evaluation set.\n",
    "We select the \"best\" parameter setting (or model) based on the validation set. We then rebuild a model using training and\n",
    "validation data with this parameter setting, and evaluate it on the test set. The test set performance serves as an estimate of the generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Load the boston housing data. Split the data into three parts, for example by calling ``train_test_split`` twice.\n",
    "As yesterday, scale the data and create polynomial features.\n",
    "Search the best setting for the regularization parameter alpha using the strategy described above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0.001     0.01      0.1       1.       10.      100.     1000.   ]\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-3, 3, 7)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing parameter search...\n",
      "       alpha        score\n",
      "    0.001000     0.698127\n",
      "    0.010000     0.698149\n",
      "    0.100000     0.698359\n",
      "    1.000000     0.700425\n",
      "   10.000000     0.717783\n",
      "  100.000000     0.759543\n",
      " 1000.000000     0.568988\n",
      "maximum score =     0.759543\n",
      "chosen alpha  =   100.000000\n",
      "Performing fit on entire training set...\n",
      "Peforming test...\n",
      "score =     0.583461\n"
     ]
    }
   ],
   "source": [
    "X_trainval, X_test, y_trainval, y_test = train_test_split(boston.data, boston.target, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_trainval = scaler.transform(X_trainval)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Performing parameter search...\")\n",
    "scores = []\n",
    "print(\"{0:>12s} {1:>12s}\".format(\"alpha\",\"score\"))\n",
    "for a in alphas:\n",
    "    ridge = Ridge(a)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    score = ridge.score(X_val, y_val)\n",
    "    scores.append(score)\n",
    "    print(\"{0:12.6f} {1:12.6f}\".format(a,score))\n",
    "    \n",
    "print(\"maximum score = {:12.6f}\".format(scores[np.argmax(scores)]))\n",
    "myalpha = alphas[np.argmax(scores)]\n",
    "print(\"chosen alpha  = {:12.6f}\".format(myalpha))\n",
    "\n",
    "print(\"Performing fit on entire training set...\")\n",
    "ridge = Ridge(myalpha)\n",
    "ridge.fit(X_trainval, y_trainval)\n",
    "print(\"Peforming test...\")\n",
    "score = ridge.score(X_test, y_test)\n",
    "print(\"score = {:12.6f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "To get a better understanding of cross-validation, we'll implement it from scratch.\n",
    "Our goal is to estimate the performance of a single model, let's say ``Ridge(alpha=1)`` on the original Boston housing dataset.\n",
    "\n",
    "### Task 2\n",
    "Complete the code below to fit a model for each of the folds of 5-fold cross-validation and compute the hold-out $R^2$ using the ``score method``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold out the following range: 0-101\n",
      "[-- -- -- -- -- -- -- -- -- -- -- -- --]\n",
      "[0.13262 0.0 8.56 0.0 0.52 5.851 96.7 2.1069 5.0 384.0 20.9 394.05 16.47]\n",
      "0.742522041858\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "score =     0.742522\n",
      "Hold out the following range: 101-202\n",
      "[0.13262 0.0 8.56 0.0 0.52 5.851 96.7 2.1069 5.0 384.0 20.9 394.05 16.47]\n",
      "[-- -- -- -- -- -- -- -- -- -- -- -- --]\n",
      "0.742522041858\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "score =     0.742522\n",
      "Hold out the following range: 202-303\n",
      "[-- -- -- -- -- -- -- -- -- -- -- -- --]\n",
      "[0.13262 0.0 8.56 0.0 0.52 5.851 96.7 2.1069 5.0 384.0 20.9 394.05 16.47]\n",
      "0.742522041858\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "score =     0.742522\n",
      "Hold out the following range: 303-404\n",
      "[-- -- -- -- -- -- -- -- -- -- -- -- --]\n",
      "[0.13262 0.0 8.56 0.0 0.52 5.851 96.7 2.1069 5.0 384.0 20.9 394.05 16.47]\n",
      "0.742522041858\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "score =     0.742522\n",
      "Hold out the following range: 404-505\n",
      "[-- -- -- -- -- -- -- -- -- -- -- -- --]\n",
      "[0.13262 0.0 8.56 0.0 0.52 5.851 96.7 2.1069 5.0 384.0 20.9 394.05 16.47]\n",
      "0.742522041858\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "[-0.40349384 -0.48832004 -0.37538041 -0.27288841 -0.29910567 -0.6179684\n",
      "  1.000203   -0.80320985 -0.52493098 -0.14549091  1.13299322  0.41034991\n",
      "  0.53341842]\n",
      "score =     0.742522\n",
      "[0.7425220418579892, 0.7425220418579892, 0.7425220418579892, 0.7425220418579892, 0.7425220418579892]\n"
     ]
    }
   ],
   "source": [
    "# TODO: problem with masking arrays\n",
    "\n",
    "X = boston.data[:505]  # we make it divisible by n_folds to make the code simpler\n",
    "y = boston.target[:505]\n",
    "scores = []\n",
    "n_folds = 5\n",
    "n_samples = len(X)\n",
    "fold_size = n_samples / n_folds\n",
    "alpha = 1.0\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    \n",
    "    X_hold_out_mask = np.zeros(X.shape, dtype=np.bool)\n",
    "    y_hold_out_mask = np.zeros(y.shape, dtype=np.bool)\n",
    "    \n",
    "    # assign True to the samples that are supposed to be held out in this fold\n",
    "    b = int((fold)*fold_size)\n",
    "    e = int((fold+1)*fold_size)\n",
    "    print(\"Hold out the following range: {0}-{1}\".format(b,e))\n",
    "    X_hold_out_mask[b:e,:] = True\n",
    "    y_hold_out_mask[b:e] = True\n",
    "    X_training_mask = ~X_hold_out_mask  # training data is inverse of hold out data\n",
    "    y_training_mask = ~y_hold_out_mask\n",
    "\n",
    "    # assign training and hold-out portions\n",
    "    X_train = np.ma.masked_array(X, X_hold_out_mask)\n",
    "    y_train = np.ma.masked_array(y, y_hold_out_mask)\n",
    "    X_test = np.ma.masked_array(X, X_training_mask)\n",
    "    y_test = np.ma.masked_array(y, y_training_mask)\n",
    "    \n",
    "    # FIXME - masking works here\n",
    "    print(X_test[105,:])\n",
    "    print(X_train[105,:])\n",
    "\n",
    "    # build model - scaling doesn't seem to be using masks!!\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    ridge = Ridge(alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    print(ridge.score(X_train, y_train))\n",
    "\n",
    "    # FIXME - but masking is now gone!\n",
    "    print(X_test[105,:])\n",
    "    print(X_train[105,:])\n",
    "\n",
    "    # compute scores\n",
    "    score = ridge.score(X_test, y_test)\n",
    "    print(\"score = {:12.6f}\".format(score))\n",
    "    scores.append(score)\n",
    "    \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "Compare the result of your implementation with the result of the ``cross_val_score`` method in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6386759   0.72005835  0.58512144  0.09886789 -0.174504  ]\n"
     ]
    }
   ],
   "source": [
    "X_train = X\n",
    "y_train = y\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "ridge = Ridge(1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "scores_sklearn = cross_val_score(ridge,X_train,y_train,cv=5)\n",
    "\n",
    "# compare scores_sklearn with scores\n",
    "print(scores_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter selection with cross-validation\n",
    "### Task 4\n",
    "Implement the same search over the parameter ``alpha`` in ``Ridge`` that you did in Task 1, but instead of splitting the data three times use cross-validation.\n",
    "In more detail:\n",
    "- Split the Boston housing data (with polynomial features) into two parts, training and testing\n",
    "- Loop over different values of alpha\n",
    "- for each value of alpha, call ``cross_val_score`` on the training set, and compute the mean cross-validated accuracy.\n",
    "- Select the parameter with the best mean crossvalidation accuracy, and build a model on all of the training data\n",
    "- evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing parameter search...\n",
      "alpha: 0.001\n",
      "Cross validated scores:\n",
      "[ 0.76238697  0.56939068  0.77952221  0.70772969  0.79034167]\n",
      "Mean cv score:     0.721874\n",
      "\n",
      "alpha: 0.01\n",
      "Cross validated scores:\n",
      "[ 0.76240138  0.56938802  0.77952438  0.7077228   0.79035472]\n",
      "Mean cv score:     0.721878\n",
      "\n",
      "alpha: 0.1\n",
      "Cross validated scores:\n",
      "[ 0.76254486  0.56936155  0.77954572  0.70765383  0.79048404]\n",
      "Mean cv score:     0.721918\n",
      "\n",
      "alpha: 1.0\n",
      "Cross validated scores:\n",
      "[ 0.76392265  0.5691061   0.77972178  0.70696038  0.79166431]\n",
      "Mean cv score:     0.722275\n",
      "\n",
      "alpha: 10.0\n",
      "Cross validated scores:\n",
      "[ 0.77374246  0.56785091  0.77921536  0.70018795  0.79683623]\n",
      "Mean cv score:     0.723567\n",
      "\n",
      "alpha: 100.0\n",
      "Cross validated scores:\n",
      "[ 0.78133063  0.59449826  0.74215667  0.65395722  0.76706821]\n",
      "Mean cv score:     0.707802\n",
      "\n",
      "alpha: 1000.0\n",
      "Cross validated scores:\n",
      "[ 0.57349832  0.53316797  0.48419302  0.42576949  0.52893181]\n",
      "Mean cv score:     0.509112\n",
      "\n",
      "best mean cross-validation score: 0.724\n",
      "best alpha  = 10.000\n",
      "Performing fit on entire training set...\n",
      "Peforming test...\n",
      "test-set score: 0.633709\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-3, 3, 7)\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Performing parameter search...\")\n",
    "scores = []\n",
    "for a in alphas:\n",
    "    print(\"alpha: {0}\".format(a))\n",
    "    ridge = Ridge(a)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    cvscore = cross_val_score(ridge,X_train,y_train,cv=5)\n",
    "    print(\"Cross validated scores:\")\n",
    "    print(cvscore)\n",
    "    score = np.mean(cvscore)\n",
    "    scores.append(score)\n",
    "    print(\"Mean cv score: {0:12.6f}\\n\".format(score))\n",
    "    \n",
    "print(\"best mean cross-validation score: {:.3f}\".format(scores[np.argmax(scores)]))\n",
    "myalpha = alphas[np.argmax(scores)]\n",
    "print(\"best alpha  = {:.3f}\".format(myalpha))\n",
    "\n",
    "print(\"Performing fit on entire training set...\")\n",
    "ridge = Ridge(myalpha)\n",
    "ridge.fit(X_trainval, y_trainval)\n",
    "print(\"Peforming test...\")\n",
    "score = ridge.score(X_test, y_test)\n",
    "print(\"test-set score: {:3f}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV\n",
    "Because searching for the parameters of a model is such a common task, scikit-learn provides ``GridSearchCV`` which implements the procedure from Task 4 (with some bells an whistles).\n",
    "To use ``GridSearchCV`` we simply have to define a parameter grid to search as a dictionary, with the key the name of the parameter, and the values the parameters we like to try. The ``GridSearchCV`` class has the same interface as the classification and regression models, and we can call ``fit`` to perform the grid-search with cross-validation. It even refits the model using the best parameters! We can then use ``predict`` or ``score`` to use the model with the best parameters, retrained on the whole training data.\n",
    "\n",
    "### Task 5\n",
    "Do the same search from Task 4 (and Task 1) again, this time using ``GridSearchCV`` (from the ``sklearn.model_selection`` module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best mean cross-validation score: 0.718\n",
      "best parameters: {'alpha': 10.0}\n",
      "test-set score: 0.627\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-3, 3, 7)\n",
    "param_grid = {'alpha':  alphas}\n",
    "grid = GridSearchCV(Ridge(), param_grid=param_grid, cv=10, return_train_score=True)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``GridSearchCV`` object stored a lot of useful information from the grid-search in the ``cv_results_`` attribute.\n",
    "The easiest way to access it is to convert it to a pandas datafram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mean_fit_time', 'mean_score_time', 'mean_test_score',\n",
       "       'mean_train_score', 'param_alpha', 'params', 'rank_test_score',\n",
       "       'split0_test_score', 'split0_train_score', 'split1_test_score',\n",
       "       'split1_train_score', 'split2_test_score', 'split2_train_score',\n",
       "       'split3_test_score', 'split3_train_score', 'split4_test_score',\n",
       "       'split4_train_score', 'split5_test_score', 'split5_train_score',\n",
       "       'split6_test_score', 'split6_train_score', 'split7_test_score',\n",
       "       'split7_train_score', 'split8_test_score', 'split8_train_score',\n",
       "       'split9_test_score', 'split9_train_score', 'std_fit_time',\n",
       "       'std_score_time', 'std_test_score', 'std_train_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     {'alpha': 0.001}\n",
       "1      {'alpha': 0.01}\n",
       "2       {'alpha': 0.1}\n",
       "3       {'alpha': 1.0}\n",
       "4      {'alpha': 10.0}\n",
       "5     {'alpha': 100.0}\n",
       "6    {'alpha': 1000.0}\n",
       "Name: params, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even plot the cross-validation accuracies and their associated uncertainties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5d238abdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results.plot('param_alpha', 'mean_train_score')\n",
    "results.plot('param_alpha', 'mean_test_score', ax=plt.gca())\n",
    "plt.fill_between(results.param_alpha.astype(np.float),\n",
    "                 results['mean_train_score'] + results['std_train_score'],\n",
    "                 results['mean_train_score'] - results['std_train_score'], alpha=0.2)\n",
    "plt.fill_between(results.param_alpha.astype(np.float),\n",
    "                 results['mean_test_score'] + results['std_test_score'],\n",
    "                 results['mean_test_score'] - results['std_test_score'], alpha=0.2)\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "Select the best value of ``n_neighbors`` for using ``KNeighborsClassifier`` on the ``digits`` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics and scoring\n",
    "\n",
    "In this section, we'll look at different evaluation metrics in scikit-learn and how to use them.\n",
    "There's two main ways to use metrics:\n",
    "- As functions in the ``sklearn.metrics`` module, such as ``accuracy_score`` and ``roc_auc``. These take the true labels and the predictions as arguments.\n",
    "- By specifying a metrics in ``cross_val_score``, ``GridSearchCV`` or another evaluation method using the ``scoring`` keyword, i.e. ``cross_val_score(..., scoring='roc_auc')``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for binary classification\n",
    "As we mentioned, accuracy is not a great metric in imbalanced classification problems.\n",
    "We'll look at some alternatives.\n",
    "\n",
    "### Task 7\n",
    "Create an imbalanced classification problem from the digits dataset by classifying the digit 4 against all other digits.\n",
    "Split the data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ....\n",
    "# create X_train, X_test, y_train, y_test for \"4 vs rest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a ``LogisticRegression`` model, a ``DummyClassifier(strategy='most_frequent')`` and a ``DecisionTreeClassifier(max_depth=2)``, and compare their test-set accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lr = LogisticRegression()\n",
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "# build models\n",
    "# compare them using accuracy (for example using .score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better picture, now use the ``classification_report`` function from ``sklearn.metrics``:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report provides precision and recall for the default threshold. To look at all possible thresholds, we can plot the precision-recall curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_probs_lr = lr.predict_proba(X_test)[:, 1]\n",
    "# complete:\n",
    "# positive_probs_tree = tree.\n",
    "# plot curves for tree and logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a summary by computing the average precision (``average_precision_score``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to use something like ``average_precision_score`` in cross-validation, we can simply specify the ``scoring`` argument of ``cross_val_score``. Use ``cross_val_score`` to compute the 5 fold cross-validated average precision of ``LogisticRegression`` and ``DecisionTreeClassifier(max_depth=2)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... solution here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
