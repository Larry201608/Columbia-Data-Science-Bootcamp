{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation and model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threefold split and parameter search\n",
    "The simplest way to adjust parameters is to split the data into three parts: a training, a validation and a test set.\n",
    "For each parameter setting, we fit a model on the training set, and evaluate it on the evaluation set.\n",
    "We select the \"best\" parameter setting (or model) based on the validation set. We then rebuild a model using training and\n",
    "validation data with this parameter setting, and evaluate it on the test set. The test set performance serves as an estimate of the generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Load the boston housing data. Split the data into three parts, for example by calling ``train_test_split`` twice.\n",
    "As yesterday, scale the data and create polynomial features.\n",
    "Search the best setting for the regularization parameter alpha using the strategy described above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-3, 3, 7)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "To get a better understanding of cross-validation, we'll implement it from scratch.\n",
    "Our goal is to estimate the performance of a single model, let's say ``Ridge(alpha=1)`` on the original Boston housing dataset.\n",
    "\n",
    "### Task 2\n",
    "Complete the code below to fit a model for each of the folds of 5-fold cross-validation and compute the hold-out $R^2$ using the ``score method``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = boston.data[:505]  # we make it divisible by n_folds to make the code simpler\n",
    "y = boston.target\n",
    "\n",
    "\n",
    "scores = []\n",
    "n_folds = 5\n",
    "n_samples = len(X)\n",
    "fold_size = n_samples // 5\n",
    "for fold in range(n_folds):\n",
    "    hold_out_mask = np.zeros(n_samples, dtype=np.bool)\n",
    "    # assign True to the samples that are supposed to be held out in this fold\n",
    "    # ...\n",
    "    training_mask = ~hold_out_mask  # training data is inverse of hold out data\n",
    "    # assign training and hold-out portions\n",
    "    # build model\n",
    "    # compute scores\n",
    "    # ...\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "Compare the result of your implementation with the result of the ``cross_val_score`` method in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_sklearn = cross_val_score() # fill in missing arguments\n",
    "\n",
    "# compare scores_sklearn with scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter selection with cross-validation\n",
    "### Task 4\n",
    "Implement the same search over the parameter ``alpha`` in ``Ridge`` that you did in Task 1, but instead of splitting the data three times use cross-validation.\n",
    "In more detail:\n",
    "- Split the Boston housing data (with polynomial features) into two parts, training and testing\n",
    "- Loop over different values of alpha\n",
    "- for each value of alpha, call ``cross_val_score`` on the training set, and compute the mean cross-validated accuracy.\n",
    "- Select the parameter with the best mean crossvalidation accuracy, and build a model on all of the training data\n",
    "- evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV\n",
    "Because searching for the parameters of a model is such a common task, scikit-learn provides ``GridSearchCV`` which implements the procedure from Task 4 (with some bells an whistles).\n",
    "To use ``GridSearchCV`` we simply have to define a parameter grid to search as a dictionary, with the key the name of the parameter, and the values the parameters we like to try. The ``GridSearchCV`` class has the same interface as the classification and regression models, and we can call ``fit`` to perform the grid-search with cross-validation. It even refits the model using the best parameters! We can then use ``predict`` or ``score`` to use the model with the best parameters, retrained on the whole training data.\n",
    "\n",
    "### Task 5\n",
    "Do the same search from Task 4 (and Task 1) again, this time using ``GridSearchCV`` (from the ``sklearn.model_selection`` module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "alphas = np.logspace(-3, 3, 7)\n",
    "param_grid = {'alpha':  alphas}\n",
    "\n",
    "grid = GridSearchCV( ,return_train_score=True) # complete me!\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "print(\"best parameters: {}\".format(grid.best_params_))\n",
    "print(\"test-set score: {:.3f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``GridSearchCV`` object stored a lot of useful information from the grid-search in the ``cv_results_`` attribute.\n",
    "The easiest way to access it is to convert it to a pandas datafram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results = pd.DataFrame(grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even plot the cross-validation accuracies and their associated uncertainties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results.plot('param_n_neighbors', 'mean_train_score')\n",
    "results.plot('param_n_neighbors', 'mean_test_score', ax=plt.gca())\n",
    "plt.fill_between(results.param_n_neighbors.astype(np.int),\n",
    "                 results['mean_train_score'] + results['std_train_score'],\n",
    "                 results['mean_train_score'] - results['std_train_score'], alpha=0.2)\n",
    "plt.fill_between(results.param_n_neighbors.astype(np.int),\n",
    "                 results['mean_test_score'] + results['std_test_score'],\n",
    "                 results['mean_test_score'] - results['std_test_score'], alpha=0.2)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "Select the best value of ``n_neighbors`` for using ``KNeighborsClassifier`` on the ``digits`` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics and scoring\n",
    "\n",
    "In this section, we'll look at different evaluation metrics in scikit-learn and how to use them.\n",
    "There's two main ways to use metrics:\n",
    "- As functions in the ``sklearn.metrics`` module, such as ``accuracy_score`` and ``roc_auc``. These take the true labels and the predictions as arguments.\n",
    "- By specifying a metrics in ``cross_val_score``, ``GridSearchCV`` or another evaluation method using the ``scoring`` keyword, i.e. ``cross_val_score(..., scoring='roc_auc')``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for binary classification\n",
    "As we mentioned, accuracy is not a great metric in imbalanced classification problems.\n",
    "We'll look at some alternatives.\n",
    "\n",
    "### Task 7\n",
    "Create an imbalanced classification problem from the digits dataset by classifying the digit 4 against all other digits.\n",
    "Split the data into training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# ....\n",
    "# create X_train, X_test, y_train, y_test for \"4 vs rest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a ``LogisticRegression`` model, a ``DummyClassifier(strategy='most_frequent')`` and a ``DecisionTreeClassifier(max_depth=2)``, and compare their test-set accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "lr = LogisticRegression()\n",
    "tree = DecisionTreeClassifier(max_depth=2)\n",
    "dummy = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "# build models\n",
    "# compare them using accuracy (for example using .score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better picture, now use the ``classification_report`` function from ``sklearn.metrics``:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report provides precision and recall for the default threshold. To look at all possible thresholds, we can plot the precision-recall curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "positive_probs_lr = lr.predict_proba(X_test)[:, 1]\n",
    "# complete:\n",
    "# positive_probs_tree = tree.\n",
    "# plot curves for tree and logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a summary by computing the average precision (``average_precision_score``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to use something like ``average_precision_score`` in cross-validation, we can simply specify the ``scoring`` argument of ``cross_val_score``. Use ``cross_val_score`` to compute the 5 fold cross-validated average precision of ``LogisticRegression`` and ``DecisionTreeClassifier(max_depth=2)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... solution here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
